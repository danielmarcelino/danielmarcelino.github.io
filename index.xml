<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data * Analysis &#43; R on Data * Analysis &#43; R</title>
    <link>https://danielmarcelino.github.io/</link>
    <description>Recent content in Data * Analysis &#43; R on Data * Analysis &#43; R</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Daniel Marcelino</copyright>
    <lastBuildDate>Wed, 03 Apr 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Be the Best at What Matters Most in Versioning Control</title>
      <link>https://danielmarcelino.github.io/2019-04-04-what-matters-most-in-versioning/</link>
      <pubDate>Wed, 03 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://danielmarcelino.github.io/2019-04-04-what-matters-most-in-versioning/</guid>
      <description>


&lt;div id=&#34;motivation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;I began to work on some shared repositories with three novice students of R and Github workflow. I’ve noticed they are very enthusiasts with R/Rstudio coding, but they simply don’t have much waist play of versioning control tools.&lt;/p&gt;
&lt;p&gt;When I was learning and aspiring to move my repositories from a centralized system called Subversion (SVN) to Github (Git)–a system that allows multiple repositories–,&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; I found myself reading several guides on Internet, including those graphical ones, hoping to memorizing every Git command line trick.&lt;/p&gt;
&lt;p&gt;In a decade or less, Git has surpassed others code management systems and became the new norm.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; That’s why learning it is relevant. However, I’ve realized that for 99% of the work I actually do, I’d have needed to get hang of only very few commands. So I decide to write them down as they may be helpful to others as well.&lt;/p&gt;
&lt;div id=&#34;find-the-version-of-git&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Find the version of Git&lt;/h4&gt;
&lt;p&gt;Run &lt;code&gt;git --version&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-a-new-git-repository&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Create a new Git repository&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Go to the folder of the project.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Run &lt;code&gt;git init&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;clone-an-existing-git-repository&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Clone an existing Git repository&lt;/h4&gt;
&lt;p&gt;Cloning is the process of pulling down a copy of a repository stored on a server.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Go to the parent folder of where you want to repository’s folder to be in.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;code&gt;git clone [url to repository&#39;s git file] [name of folder / repository you want]&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;check-the-status-of-a-git-repository&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Check the status of a Git repository&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git status&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tell-git-to-track-a-file-named-readme.md&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Tell Git to track a file named &lt;code&gt;README.md&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git add README.md&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tell-git-to-track-a-whole-folder-named-script&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Tell Git to track a whole folder named &lt;code&gt;script&lt;/code&gt;&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git add scripts/&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tell-git-to-track-and-stage-all-files-and-subfolders-in-a-directory&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Tell Git to track (and stage) all files and subfolders in a directory&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git add -A&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;make-a-commit&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Make a commit&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git commit --m &#34;my changes commit&#34;&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;view-all-branches&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;View all branches&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git branch&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-a-new-branch&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Create a new branch&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git branch new_model&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;switch-to-a-branch&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Switch to a branch&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git checkout new_model&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-a-new-branch-and-switch-to-it&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Create a new branch and switch to it&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git checkout -b new_ux&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;merge-one-branch-into-another&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Merge one branch into another&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Switch to the branch you want to pull changes into: &lt;code&gt;git checkout master&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Pull changes from another branch into your branch: &lt;code&gt;git merge new_ux&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;set-a-remote-github-repository&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Set a remote Github repository&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Go to GitHub.com and create a new repository.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Set that repository’s url as the origin repo: &lt;code&gt;git remote add origin https://github.com/danielmarcelino/myfiles.git&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;push-master-branch-to-a-github-repository&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Push master branch to a Github repository&lt;/h4&gt;
&lt;p&gt;The &lt;code&gt;-u&lt;/code&gt; sets the origin as the default for this branch&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git push -u origin master&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pull-down-from-branch-from-github-repository-to-local-repository&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Pull down from branch from GitHub repository to local repository&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git pull origin master&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pull-down-all-branches-from-github&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Pull down all branches from GitHub&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git fetch origin&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;view-all-remote-branches&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;View all remote branches&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git branch --remote&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;view-log&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;View log&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git log&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;view-unstagged-changes-to-files&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;View unstagged changes to files&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git diff&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;unstage-a-file&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Unstage a file&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git reset filename&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;undo-last-commit-move-commits-changes-to-staging&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Undo last commit, move commits changes to staging&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git reset --soft HEAD^&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;undo-last-commit-remove-all-changes-in-your-working-directory&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Undo last commit, remove all changes in your working directory&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git reset --hard HEAD^&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;clone-a-remote-repository-locally&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Clone a remote repository locally&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git clone url&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;git-tags&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Git tags&lt;/h4&gt;
&lt;p&gt;Tags in Git can be used to add a version number. Every new commit after this will auto incremented by appending commit number and hash to the tag. The tag must follow the pattern of &lt;code&gt;X.Y.Z or&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git tag -a &#34;v1.5.0-beta&#34; -m &#34;version v1.5.0-beta&#34;&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;show-changes-from-a-particular-commit&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Show changes from a particular commit&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git show --pretty=&#34;format:&#34; &amp;lt;commit ID&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;git show v1.5.0-beta&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;revert-a-commit-by-creating-a-new-commit-with-opposite-changes&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Revert a commit by creating a new commit with opposite changes&lt;/h4&gt;
&lt;p&gt;&lt;code&gt;git revert &amp;lt;commit ID&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;To be even more precise, it’s a central repository with a series of local repositories.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://www.slideshare.net/IanSkerrett/eclipse-community-survey-2014&#34; class=&#34;uri&#34;&gt;https://www.slideshare.net/IanSkerrett/eclipse-community-survey-2014&lt;/a&gt;&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Extreme non-viable candidates and quota maneuvering in Brazilian legislative elections</title>
      <link>https://danielmarcelino.github.io/publication/2019-extreme-non-viable-candidates/</link>
      <pubDate>Fri, 01 Mar 2019 00:00:00 -0300</pubDate>
      
      <guid>https://danielmarcelino.github.io/publication/2019-extreme-non-viable-candidates/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bolsonaro&#39;s First Job Approval Ratings</title>
      <link>https://danielmarcelino.github.io/post/2019-02-26-presidential-job-approval-ratings/</link>
      <pubDate>Tue, 26 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://danielmarcelino.github.io/post/2019-02-26-presidential-job-approval-ratings/</guid>
      <description>


&lt;p&gt;President Jair Bolsonaro’s job approval ratings average 39.5% during his first quarter in office so far (from January through late February). Compared to the former presidents, for which I’ve estimates, his quarterly job approval ratings are above the overall average for the inauguration term (31%). However, his ratings trail quarterly averages of the Workers’ Party presidents, Lula I and II, and Dilma I.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Quarter   President               % Approving
--------  ---------------------  ------------
  1987.2  José Sarney                   9.0
  1990.2  Fernando Collor              36.0
  1992.3  Itamar Franco                18.0
  1995.1  Fernando Henrique I          39.0
  1999.1  Fernando Henrique II         21.0
  2003.1  Lula da Silva I              42.5
  2007.1  Lula da Silva II             48.0
  2011.1  Dilma Rousseff I             51.5
  2015.1  Dilma Rousseff II            14.7
  2016.2  Michel Temer                 19.4
  2019.1  Jair Bolsonaro               39.5&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>ANOVA Simplified Using R</title>
      <link>https://danielmarcelino.github.io/project/anova/</link>
      <pubDate>Thu, 27 Dec 2018 00:00:00 -0200</pubDate>
      
      <guid>https://danielmarcelino.github.io/project/anova/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Soccer Predictions Using Bayesian Mixed Effects Models</title>
      <link>https://danielmarcelino.github.io/project/soccer-predictions/</link>
      <pubDate>Thu, 27 Dec 2018 00:00:00 -0200</pubDate>
      
      <guid>https://danielmarcelino.github.io/project/soccer-predictions/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reading List Faster With parallel, doParallel, and pbapply</title>
      <link>https://danielmarcelino.github.io/post/2018-12-12-reading-list-faster/</link>
      <pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://danielmarcelino.github.io/post/2018-12-12-reading-list-faster/</guid>
      <description>


&lt;p&gt;I have several tables that I would like to load as a sole data frame. Derived functions from &lt;code&gt;read.table()&lt;/code&gt; have a lot of convenient features, but it seems like there is a lot of steps in the implementation that would slow things down. The gain in performance of reading 29 CSV files (about 2.2 GB) shows quite different picture. While the parallelization process does bring some improvement considering the ‘user time’, i.e. the CPU time charged for the process execution at the machine level, the ‘elapsed time’, i.e. the ‘real’ elapsed time since the process was started doesn’t show much difference. Let’s go through it.&lt;/p&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;list_of_datasets &amp;lt;- list.files(pattern = &amp;quot;*.csv&amp;quot;)

list_of_datasets&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;function&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Function&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(pbapply)
library(parallel)
library(doParallel)
library(dplyr)


#&amp;#39; Reads a list of datasets
#&amp;#39; @param x A list of datasets (names of datasets are strings)
#&amp;#39; @param func A function, the read function to use to read the data
#&amp;#39; @param parallelize Parallelize the code
#&amp;#39; @param ... Further arguments passed to func

readListFaster &amp;lt;- function(x,  func, ..., parallelize = FALSE, rbind = FALSE){

  stopifnot(length(x) &amp;gt; 0)

  read_and_assign &amp;lt;- function(dataset, func){
    dataset_name &amp;lt;- as.name(dataset)
    dataset_name &amp;lt;- func(dataset, ...)
  }

  if (parallelize) {
    message(&amp;quot;Reading in data in parallel&amp;quot;)
    clusters &amp;lt;- parallel::detectCores() %&amp;gt;%
      parallel::makeCluster()

    doParallel::registerDoParallel(clusters)

    output &amp;lt;- invisible( # invisible is used to suppress the unneeded output
      pbapply::pblapply(x,
                        read_and_assign,
                        func = func,
                        ...,
                        cl = clusters)
    )

    parallel::stopCluster(clusters)

  } else if (!parallelize) {
    output &amp;lt;- invisible(
      pbapply::pblapply(x,
                        read_and_assign,
                        func = func)
    )
  }

  # Remove what&amp;#39;s after the &amp;quot;.&amp;quot; at the end of the data set names and what&amp;#39;s before any / for url files.
  x &amp;lt;- stringr::str_replace_all(x,&amp;quot;.*/|\\..*&amp;quot;, &amp;quot;&amp;quot;)

  names(output) &amp;lt;- x

  if (rbind) {
  rbindlist(mclapply(list_of_datasets, fread, mc.cores = detectCores()), fill=TRUE)
    # dplyr::bind_rows(output)
  } else {
    output
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;div id=&#34;without-paralelize&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Without Paralelize&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(loaded_datasets1 &amp;lt;- readListFaster(list_of_datasets, 
                                 func = read.csv2, 
                                 parallelize=FALSE, 
                                 rbind = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;with-paralelize&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;With Paralelize&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;system.time(loaded_datasets2 &amp;lt;- readListFaster(list_of_datasets, 
                                 func = read.csv2, 
                                 parallelize=TRUE, 
                                 rbind = TRUE))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s important to realise that every bit of optimisation matters. But it would not help us if the outcome data frames were different, don’t you agree? Luckily all 174 million records do match.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;
table(loaded_datasets1 == loaded_datasets2)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Cartography and Spatial Analysis</title>
      <link>https://danielmarcelino.github.io/project/mapping/</link>
      <pubDate>Wed, 10 Oct 2018 00:00:00 -0300</pubDate>
      
      <guid>https://danielmarcelino.github.io/project/mapping/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Will Brazil Goes to the Instant Runoff Election?</title>
      <link>https://danielmarcelino.github.io/post/2018-10-06-will-brazil-goes-to-the-instant-runoff-election/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://danielmarcelino.github.io/post/2018-10-06-will-brazil-goes-to-the-instant-runoff-election/</guid>
      <description>


&lt;div id=&#34;alea-jacta-est&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Alea Jacta Est&lt;/h2&gt;
&lt;p&gt;This is the very last analysis before the election. A far-right nationalist candidate, Jair Messias Bolsonaro, is leading the polls with about 40% of the intentions, while the runner up candidate, Fernando Haddad, of a leftist coalition has about 25%; all the others have 35% in total.&lt;/p&gt;
&lt;p&gt;A very intriguing debate put forward in the press in the last days was if the next president would be elected right way in the primary election round. So, will the Bandwagon, shy Tory, and something else effect help electing the far-right nationalist candidate by the absolute-majority criterion vote? Following a similar strategy presented before &lt;a href=&#34;http://danielmarcelino.github.io/2014/bayes-says-dont-worry-about-scotlands-referendum.html&#34;&gt;here&lt;/a&gt; and &lt;a href=&#34;http://danielmarcelino.github.io/2014/frente-amplio-winnability.html&#34;&gt;here&lt;/a&gt;, Bayes says &lt;code&gt;don&#39;t worry&lt;/code&gt; about Bolsonaro’s victory by now.&lt;/p&gt;
&lt;p&gt;Although polling houses are showing Bolsonaro’s support augmenting systematically over the last weeks, it’s fair to remember that pollsters did a very poor job in fielding the true vote share last elections. For instance, in 2014 the main polling firms mis-predicted both Dilma Rousseff’s and Aecio Neves’ true positions by saying Dilma was to win a majority with a margin, but the decision went to a instant runoff between these two candidates.&lt;/p&gt;
&lt;p&gt;The following numbers represent the forecast with polling data made available over the last three days. Since there is a considerable number of swing voters in these polls, I did some math by distributing these undecideds before computing the final likely results. It’s a simplified simulation exercice as I do not account for time trends, house effects etc. I’m only accounting for the sample sizes.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The data&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;           Bolsonaro Haddad  Ciro Others Swing Wasting     N
Datafolha      0.360  0.220 0.130  0.200 0.040   0.050 19552
Ibope          0.360  0.220 0.110  0.180 0.050   0.080  3010
MDA            0.367  0.240 0.099  0.155 0.060   0.078  2002
Ipespe         0.360  0.220 0.110  0.290 0.020   0.000  2000
VoxPopuli      0.340  0.270 0.110  0.130 0.070   0.080  2000
ParanaPesq     0.349  0.218 0.094  0.171 0.046   0.120  1080&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;poll-of-polls&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Poll of polls&lt;/h2&gt;
&lt;p&gt;Here is where the magic begins. I weigh polls so to reflect their sample sizes. The new results are shown in last line (7) of the table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(digits=3)

wtd.polls &amp;lt;- rbind(data, c(apply(data[,1:6],2, weighted.mean, data$N), sum(data$N)))

print(wtd.polls)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;           Bolsonaro Haddad  Ciro Others  Swing Wasting     N
Datafolha      0.360  0.220 0.130  0.200 0.0400  0.0500 19552
Ibope          0.360  0.220 0.110  0.180 0.0500  0.0800  3010
MDA            0.367  0.240 0.099  0.155 0.0600  0.0780  2002
Ipespe         0.360  0.220 0.110  0.290 0.0200  0.0000  2000
VoxPopuli      0.340  0.270 0.110  0.130 0.0700  0.0800  2000
ParanaPesq     0.349  0.218 0.094  0.171 0.0460  0.1200  1080
7              0.359  0.225 0.122  0.195 0.0433  0.0561 29644&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adjusting-for-the-undecideds&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adjusting for the undecideds&lt;/h2&gt;
&lt;p&gt;Adjusting for swing voters, the new results are now the line (8) of the table.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(digits=3)

wtd.polls[8,] &amp;lt;- data.frame(wtd.polls[7,1:4] +
                              wtd.polls[7,1:4] / 
                              sum(wtd.polls[7,1:4]) * 
                              wtd.polls[7,5], 
                              Swing=0, 
                              Wasting=wtd.polls[7,6], 
                              N=wtd.polls[7,7])
print(wtd.polls)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;           Bolsonaro Haddad  Ciro Others  Swing Wasting     N
Datafolha      0.360  0.220 0.130  0.200 0.0400  0.0500 19552
Ibope          0.360  0.220 0.110  0.180 0.0500  0.0800  3010
MDA            0.367  0.240 0.099  0.155 0.0600  0.0780  2002
Ipespe         0.360  0.220 0.110  0.290 0.0200  0.0000  2000
VoxPopuli      0.340  0.270 0.110  0.130 0.0700  0.0800  2000
ParanaPesq     0.349  0.218 0.094  0.171 0.0460  0.1200  1080
7              0.359  0.225 0.122  0.195 0.0433  0.0561 29644
8              0.376  0.235 0.128  0.205 0.0000  0.0561 29644&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;adjusting-for-the-wasting-votes&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adjusting for the wasting votes&lt;/h2&gt;
&lt;p&gt;Adjusting for wasting votes, follows the same principle. The last line of the following tbale (9) has the new adjusted preference distribution, with correct sample size.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;options(digits=3)

wtd.polls[9,] &amp;lt;- data.frame(wtd.polls[8,1:4] +
                              wtd.polls[8,1:4] / 
                              sum(wtd.polls[8,1:4]) * 
                              wtd.polls[8,6], 
                              Swing=0, 
                              Wasting=0, 
                              N=(wtd.polls[8,7] - (wtd.polls[8,6] * wtd.polls[8,7])))

print(wtd.polls)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;           Bolsonaro Haddad  Ciro Others  Swing Wasting     N
Datafolha      0.360  0.220 0.130  0.200 0.0400  0.0500 19552
Ibope          0.360  0.220 0.110  0.180 0.0500  0.0800  3010
MDA            0.367  0.240 0.099  0.155 0.0600  0.0780  2002
Ipespe         0.360  0.220 0.110  0.290 0.0200  0.0000  2000
VoxPopuli      0.340  0.270 0.110  0.130 0.0700  0.0800  2000
ParanaPesq     0.349  0.218 0.094  0.171 0.0460  0.1200  1080
7              0.359  0.225 0.122  0.195 0.0433  0.0561 29644
8              0.376  0.235 0.128  0.205 0.0000  0.0561 29644
9              0.398  0.249 0.135  0.217 0.0000  0.0000 27980&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wtd.polls$N[9] *
  c(wtd.polls$Bolsonaro[9], (wtd.polls$Haddad[9] + wtd.polls$Ciro[9] + wtd.polls$Others[9]), 1 - wtd.polls$Bolsonaro[9] - (wtd.polls$Haddad[9] - wtd.polls$Ciro[9] - wtd.polls$Others[9]))+1&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;[1] 11146 16832 19708&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;draw-1-million-samples&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Draw 1 million samples&lt;/h2&gt;
&lt;p&gt;Finally I draw a lot os samples from the posterior distribution using the weighted polls and uninformative priors to keep it simple.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;poll &amp;lt;- c(4910, 3011, 1756, 2718)

library(SciencesPo)
library(MCMCpack)


### draw samples from the posterior
set.seed(1234)
MC &amp;lt;- 1000000

### Using uninformative prior (1,1,1,1)
#samples &amp;lt;- getDirichletSamples(MC, alpha = poll + rep(1,4))  


row= 9
prob2win = function(row, export=1){
  p=rdirichlet(100000,
  wtd.polls$N[row] *
  c(wtd.polls$Bolsonaro[row], wtd.polls$Haddad[row] + wtd.polls$Ciro[row] + wtd.polls$Others[row], 1 - wtd.polls$Bolsonaro[row] - wtd.polls$Haddad[row] - wtd.polls$Ciro[row] - wtd.polls$Others[row])+1)
  if(export==1){
    mean(p[,1]&amp;lt;p[,2]) ## No exceeds Yes?
  } else {
    return(p)
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we want to look at the margins of Bolsonaro over the combined opposition candidates. The more candidates contesting for the seat, the greater the probability that the winning candidate will receive only a minority of the votes cast.&lt;/p&gt;
&lt;p&gt;We can also use the middle 95% range to represent the uncertainty. The numbers say about 20% of times in 1 million elections Bolsonaro appears ahead the opposition formula. Therefore, it’s very unlikely he could win the election in the primary election round.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;samples = prob2win(row= 9, export=0)

combinedOpposition &amp;lt;- (samples[,2])
frontRunner &amp;lt;- (samples[,1])

margin &amp;lt;- (combinedOpposition - frontRunner)

quantile(margin, probs = c(0.025, 0.5, 0.975))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; 2.5%   50% 97.5% 
0.192 0.203 0.215 &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, we can plot the posterior distribution of simulated elections where Bolsonaro is greater than the combined opposing votes. Based on the polling data at hands, and very little effort, we can believe the far-right nationalist candidate won’t make it this Sunday as quite a few press pundits are suggesting.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; hist(margin, 
      col=&amp;quot;gray&amp;quot;,
      prob = FALSE, # posterior distribution
      breaks = &amp;quot;FD&amp;quot;, xlab = expression(p[Bolsonaro] &amp;gt; p[Opposition]),
      main = expression(paste(bold(&amp;quot;Posterior Distribution of Elections With &amp;quot;),  p[Bolsonaro] &amp;gt; p[Opposition])));
# Bayes estimate (middle 95%)
abline(v=mean(margin), col=&amp;#39;red&amp;#39;, lwd=3, lty=3);&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;simulation.png&#34; alt=&#34;Posterior Distribution of Simulated Elections&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Posterior Distribution of Simulated Elections&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Example Page</title>
      <link>https://danielmarcelino.github.io/tutorial/example/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 -0300</pubDate>
      
      <guid>https://danielmarcelino.github.io/tutorial/example/</guid>
      <description>

&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;

&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;

&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Políticas Públicas de Esporte, Lazer e Juventude Baseadas em  Evidências</title>
      <link>https://danielmarcelino.github.io/publication/2017-politicas-publicas-baseadas-em-evidencias/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 -0300</pubDate>
      
      <guid>https://danielmarcelino.github.io/publication/2017-politicas-publicas-baseadas-em-evidencias/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Game of Thrones Analysis</title>
      <link>https://danielmarcelino.github.io/project/got/</link>
      <pubDate>Thu, 15 Jun 2017 00:00:00 -0300</pubDate>
      
      <guid>https://danielmarcelino.github.io/project/got/</guid>
      <description></description>
    </item>
    
    <item>
      <title>May the Force of R be With You, Always!</title>
      <link>https://danielmarcelino.github.io/post/2017-01-02-may-the-force-of-r-be-with-you-always/</link>
      <pubDate>Mon, 02 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>https://danielmarcelino.github.io/post/2017-01-02-may-the-force-of-r-be-with-you-always/</guid>
      <description>


&lt;p&gt;With a Telegram account connected to &lt;code&gt;@TeleR&lt;/code&gt;, the force of R can always be with me, where I have data. The following is a screenshot of my mobile:&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;telegram_plot.png&#34; alt=&#34;R Force&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;R Force&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;If you want to have R where you are too, you only need a &lt;a href=&#34;https://telegram.org/&#34;&gt;Telegram&lt;/a&gt; account; then, you have to search for the public username &lt;code&gt;@Tele_R&lt;/code&gt;, starting a new chat with. Thanks to &lt;a href=&#34;http://telemath.altervista.org/TeleR.html&#34;&gt;TeleMath Team&lt;/a&gt;, you will be able to send R commands as messages and get answers back.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Effective Number of Parties in the Electorate by Year and Region</title>
      <link>https://danielmarcelino.github.io/post/2016-11-26-effective-number-of-parties/</link>
      <pubDate>Sun, 27 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://danielmarcelino.github.io/post/2016-11-26-effective-number-of-parties/</guid>
      <description>


&lt;div id=&#34;tasks-compute-the-effective-number-of-parties-by-year-and-region&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tasks: Compute the Effective Number of Parties by year and region&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Using the American National Election Studies, compute the Effective Number of Parties in the electorate across regions and years/waves.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The data&lt;/h2&gt;
&lt;p&gt;The data come from The American National Election Studies (ANES). The &lt;a href=&#34;http://www.electionstudies.org/&#34;&gt;ANES&lt;/a&gt; is a survey that covers voting behavior, public opinion, and political participation. Many other countries have their own version of this survey, for instance see &lt;a href=&#34;http://ellisp.github.io/blog/2016/09/18/nzes1&#34;&gt;here&lt;/a&gt;. While the primary mission of these studies is to answer questions about voting behavior, the wealth of variables collected amongst voters means that we can use these data to answer other questions too. If you would like to know about the other variables contained in the ANES questionnaires, you may want to read its &lt;a href=&#34;http://www.electionstudies.org/studypages/anes_timeseries_cdf/anes_timeseries_cdf_codebook_var.pdf&#34;&gt;codebook&lt;/a&gt;. You may also be interested in a &lt;a href=&#34;http://www.asdfree.com/2013/11/analyze-american-national-election.html&#34;&gt;post&lt;/a&gt; by Anthony Damico on this topic.&lt;/p&gt;
&lt;p&gt;There are many ways you can import the version of data I’ve made. For example, you download it. Then read it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(ggplot2)
library(RCurl)
library(SciencesPo) &lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;download.file(&amp;quot;https://github.com/danielmarcelino/Datasets/raw/master/ANES/anes-data.RData&amp;quot;, destfile = &amp;quot;anes-data.RData&amp;quot;)

ANES &amp;lt;- readRDS(&amp;quot;anes-data.RData&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;data-chasing&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data chasing&lt;/h2&gt;
&lt;p&gt;I’m particularly interested in 3 variables from the list. Party identification (VCF0301), originally measured as 7-point scale, the census region code (VCF0112), and years of the wave (VCF0004).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ANES$PID3 &amp;lt;- factor(ANES$VCF0301) # Convert to three-level Party ID:
levels(ANES$PID3) &amp;lt;- c(&amp;quot;Dem&amp;quot;, &amp;quot;Dem&amp;quot;, &amp;quot;Dem&amp;quot;, &amp;quot;Ind&amp;quot;, &amp;quot;Rep&amp;quot;, &amp;quot;Rep&amp;quot;, &amp;quot;Rep&amp;quot;)

ANES$Region &amp;lt;- factor(ANES$VCF0112) # # Convert to factor
levels(ANES$Region) &amp;lt;- c(&amp;quot;Northeast&amp;quot;, &amp;quot;North Central&amp;quot;, &amp;quot;South&amp;quot;, &amp;quot;West&amp;quot;)

names(ANES)[names(ANES)==&amp;quot;VCF0004&amp;quot;] &amp;lt;- &amp;quot;Year&amp;quot;

# ANES &amp;lt;- ANES[(which(ANES$VCF0004 %in% c(2000, 2002, 2004, 2008,2012))),]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-effective-number-of-parties&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Effective Number of Parties&lt;/h2&gt;
&lt;p&gt;For those who do not know the concept of an “Effective Number of Parties”, you can read a &lt;a href=&#34;http://danielmarcelino.github.io/blog/2014/A-bit-more-fragmented.html&#34;&gt;post back in 2014&lt;/a&gt;, or go to &lt;a href=&#34;https://en.wikipedia.org/wiki/Effective_number_of_parties&#34;&gt;Wikipedia&lt;/a&gt; for summarizing details. In short, the effective number of parties is the number of viable or important political parties in a party system that includes parties of unequal size. This measure is given by the inverse of the Herfindahl-Hirschman Index (HHI) or the inverse participation ratio (IPR) in physics. The HHI is calculated by taking the voting share of each party in the electorate, squaring them, and summing the result: &lt;span class=&#34;math inline&#34;&gt;\(HHI = s1^2 + s2^2 + s3^2 + ... + sn^2\)&lt;/span&gt; (where &lt;em&gt;s&lt;/em&gt; is the voting share of each party expressed as a whole number. In mathematical notation, it looks like:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[HHI = \sum_{i=1}^N s_i^2\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For now, I’ll be using &lt;em&gt;dplyr&lt;/em&gt; to estimate the effective number of parties by year and region, but the &lt;strong&gt;SciencesPo&lt;/strong&gt; package has a function named &lt;em&gt;politicalDiversity&lt;/em&gt; that can calculate several indices used by political science scholars, which I’ll be addressing in future posts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ENPpid3 &amp;lt;- ANES %&amp;gt;% 
group_by(Year, Region) %&amp;gt;%
  summarise(invHHI = sum(table(PID3))^2 / sum(table(PID3)^2)) %&amp;gt;%
filter(!is.na(Region))&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(ENPpid3)

Source: local data frame [112 x 3]
Groups: Year [28]

    Year        Region   invHHI
   &amp;lt;dbl&amp;gt;        &amp;lt;fctr&amp;gt;    &amp;lt;dbl&amp;gt;
1   1952     Northeast 2.251207
2   1952 North Central 2.231502
3   1952         South 1.538712
4   1952          West 2.119764
5   1956     Northeast 2.352651
6   1956 North Central 2.419891
7   1956         South 1.816944
8   1956          West 2.395399
9   1958     Northeast 2.296220
10  1958 North Central 2.247543
# ... with 102 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;the-plot&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The plot&lt;/h2&gt;
&lt;p&gt;The index takes into consideration the relative size distribution of the parties (actually, declared partisanship) in an electorate. It approaches 1 when the distribution of preferences in a region is concentrated around only one party. Conversely, the index increases when the number of parties favored in the region increases.&lt;/p&gt;
&lt;p&gt;The analysis suggests a highly concentrated political market in-the-electorate in the South states, around 1 and a half party in the 50s, but then regressing towards the national mean (invHHI = 2.349) after the 60s.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gg &amp;lt;- ggplot(ENPpid3)
gg &amp;lt;- gg + geom_line(aes(x = Year, y = invHHI, colour = Region), size=1.1)
gg &amp;lt;- gg + scale_x_continuous(limits=c(1952, 2012), 
breaks =  round(seq(1952, 2012, by = 4),1)) 
gg &amp;lt;- gg + scale_y_continuous(limits=c(1, 3))
gg &amp;lt;- gg + theme_pub()
gg &amp;lt;- gg + labs(title = &amp;quot;Effective Number of Parties-in-the-Electorate&amp;quot;, 
x = &amp;quot;Year of the survey&amp;quot;, y = &amp;quot;Index&amp;quot;)
gg&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;featured.png&#34; alt=&#34;Effective number of parties&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Effective number of parties&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;It’s true that majority plurality (single ballot) electoral systems tend to have a low number of effective parties compared to majority second ballot systems and to proportional representation systems. With an average count of effective parties in-the-electorate around 2, there is not much room for a third political force to emerge; and this seems to be quite consolidated among regions.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Data Science and Society: The SciencesPo R Package</title>
      <link>https://danielmarcelino.github.io/talk/2016-11-16-data-sciences-and-society/</link>
      <pubDate>Wed, 16 Nov 2016 00:00:00 -0200</pubDate>
      
      <guid>https://danielmarcelino.github.io/talk/2016-11-16-data-sciences-and-society/</guid>
      <description>&lt;p&gt;This presentation is part of a coordinated edition seminar, &lt;em&gt;Data Science and Society&lt;/em&gt;, organized by IBPAD and Núcleo de Estudos e Pesquisa em Políticas Públicas, Governo e Gestão (NP3-UnB).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Yet the Worst Olympic Chart</title>
      <link>https://danielmarcelino.github.io/post/2016-08-06-yet-the-worst-olympic-chart/</link>
      <pubDate>Sat, 06 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>https://danielmarcelino.github.io/post/2016-08-06-yet-the-worst-olympic-chart/</guid>
      <description>


&lt;p&gt;Ah, the numbers! The Olympic Games are back in high style in Rio. Despite Brazil’s sluggish economy and unfulfilled promises for this Summer Olympic Games, I’d say, my expectations were met yesterday with such a beautiful opening ceremony. Let’s follow the competition over the next few days–the pressure to perform has only just begun.&lt;/p&gt;
&lt;p&gt;One interesting aspect about the Olympic games is the way people around the globe cover it. In fact, several news houses are quite excited about telling predictions and showing medals rankings of all sort, starting soon today. Surprisingly, yesterday my eyes catch a stacked bar chart by &lt;a href=&#34;http://www.nbcolympics.com/&#34;&gt;NBC&lt;/a&gt; with a really bad visualization taste. It’s so inaccurate that it deserves the title of &lt;em&gt;Worst Olympic Chart&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;featured.png&#34; alt=&#34;A chart from NBC Olympics&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;A chart from NBC Olympics&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The colors are fine, but the dimensions are simply misleading. How have the 976 USA Olympic gold medals been given less length in the plot than 797 Russia’s medals (silver and bronze), or all 777.5 medals of Great Britain? Yep, that’s right; one can get half a medal by tying for a placement. Anyway, I believe it shouldn’t be that difficult to make a decent-looking, yet accurate plot for readers. But perhaps the worst is yet to come.&lt;/p&gt;
&lt;div id=&#34;top-3-all-time-olympic-medals&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Top-3 all-time Olympic medals&lt;/h1&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;olympicmedals.png&#34; /&gt;

&lt;/div&gt;
&lt;div id=&#34;the-data-points&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;The data points&lt;/h4&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt; library(dplyr)     

 country = c(rep(&amp;#39;US&amp;#39;,3),rep(&amp;#39;RUS&amp;#39;,3),rep(&amp;#39;GB&amp;#39;,3))      
 Medal = rep(c(&amp;#39;Gold&amp;#39;,&amp;#39;Silver&amp;#39;,&amp;#39;Bronze&amp;#39;),3)     
 counts = c(976.0, 759.5, 668.5, 440.0, 357.0,326.0, 233.5, 272.5, 271.5)       

 Olympics = as.data.frame(      
 cbind(country,     
  Medal,        
 counts))       

 Olympics$counts = as.numeric(levels(Olympics$counts))[Olympics$counts]     
Olympics$Medal &amp;lt;- factor(Olympics$Medal,levels = c(&amp;#39;Gold&amp;#39;,&amp;#39;Silver&amp;#39;,&amp;#39;Bronze&amp;#39;))       

Olympics &amp;lt;- Olympics %&amp;gt;%        
 group_by(country) %&amp;gt;%      
 mutate(mid_y=cumsum(counts) - 0.5*counts)      
 &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;do-the-plot&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Do the plot&lt;/h4&gt;
&lt;p&gt;Updated: to work properly, please install the ggplot2 development version, otherwise delete the theme setting line.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2) # devtools::install_github(&amp;#39;haddley/ggplot2&amp;#39;)  
library(SciencesPo) # for the theme:
devtools::install_github(&amp;#39;danielmarcelino/SciencesPo&amp;#39;)      

g &amp;lt;- ggplot(Olympics,aes(x = country, y=counts, fill = Medal))  
g &amp;lt;- g + geom_bar(stat = &amp;#39;identity&amp;#39;)        
g &amp;lt;- g + scale_fill_manual(values = c(&amp;#39;#FFD700&amp;#39;,&amp;#39;#C0C0C0&amp;#39;,&amp;#39;#CD7F32&amp;#39;))       
g &amp;lt;- g + scale_y_continuous(limits = c(0, 2500))        
g &amp;lt;- g + geom_text(aes(label=counts, y = mid_y), size = 3)      
g &amp;lt;- g + labs(x=&amp;#39;&amp;#39;,y=&amp;#39;Number of Medals&amp;#39;, title=&amp;#39;Olympic Medals&amp;#39;)
g &amp;lt;- g + coord_flip()       
# g &amp;lt;- g + theme_scipo(base_size = 13)      
g &amp;lt;- g + no_y_gridlines()       
print(g)        &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
